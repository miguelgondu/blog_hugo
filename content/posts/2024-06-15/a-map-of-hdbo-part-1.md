# A map of high-dimensional Bayesian Optimization: introduction

> Check the paper

> Check the previous two blogposts

> This blogpost is the first part of a series

## Intro

[high-dimensional BO is pretty important nowadays: AutoML, self-driving labs, drug discovery, protein engineering]

[The pie, noticing how we discussed one of the methods in the previous blogpost]

[We recently wrote a survey of discrete HDBO, but it actually covers most of the continuous case too]

## Our focus: discrete sequence optimization

[formalize the problem of discrete sequence optimization]

[Provide a couple of examples that are highly relevant]

## A (not so) toy high-dimensional example

[smb]

[Optimizing smb is actually pretty hard]

[This problem is pretty similar to actual real-life settings: "high" experiment time]

## The "worst" baseline

[Sampling tokens at random]

[plot with a line going up or down]

## A less silly baseline

[Evolving the best level through random sampling]

## Can we optimize using Bayesian Optimization?

[One-hot representations of SMB]

[Can vanilla BO find anything in one-hot space?]

[[most likely not, check the results of that experiment]]

## What comes next?

[Several strategies are being proposed for dealing with high-dimensional inputs, and in the following blogposts we'll discuss each one.]

[Pointing back to the pie, and adding a brief explanation of each family in the taxonomy]
